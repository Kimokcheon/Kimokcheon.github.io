<html>
<head>
    <meta charset="utf-8"/>
<meta name="description" content=""/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>

<title>Pytorch_Advance | Yuchuan&#39;s Blog</title>

<link rel="shortcut icon" href="https://kimokcheon.github.io//favicon.ico?v=1695778991029">

<link href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://kimokcheon.github.io//styles/main.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.0/dist/css/bootstrap.min.css">

<script src="https://cdn.jsdelivr.net/npm/@highlightjs/cdn-assets/highlight.min.js"></script>
<script src="https://cdn.bootcss.com/highlight.js/9.15.10/languages/dockerfile.min.js"></script>
<script src="https://cdn.bootcss.com/highlight.js/9.15.10/languages/dart.min.js"></script>

<script src="https://cdn.jsdelivr.net/npm/moment@2.27.0/moment.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.0/dist/js/bootstrap.min.js"></script>
<!-- DEMO JS -->
<!--<script src="media/scripts/index.js"></script>-->



    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.css">
</head>
<body>
<div class="main gt-bg-theme-color-first">
    <nav class="navbar navbar-expand-lg">
    <div class="navbar-brand">
        <img class="user-avatar" src="/images/avatar.png" alt="头像">
        <div class="site-name gt-c-content-color-first">
            Yuchuan&#39;s Blog
        </div>
    </div>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <i class="fas fa-bars gt-c-content-color-first" style="font-size: 18px"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <div class="navbar-nav mr-auto" style="text-align: center">
            
                <div class="nav-item">
                    
                        <a href="/" class="menu gt-a-link">
                            首页
                        </a>
                    
                </div>
            
                <div class="nav-item">
                    
                        <a href="/archives" class="menu gt-a-link">
                            归档
                        </a>
                    
                </div>
            
                <div class="nav-item">
                    
                        <a href="/tags" class="menu gt-a-link">
                            标签
                        </a>
                    
                </div>
            
                <div class="nav-item">
                    
                        <a href="https://kimokcheon.github.io/post/about-me" class="menu gt-a-link">
                            关于
                        </a>
                    
                </div>
            
        </div>
        <div style="text-align: center">
            <form id="gridea-search-form" style="position: relative" data-update="1695778991029" action="/search/index.html">
                <input class="search-input" autocomplete="off" spellcheck="false" name="q" placeholder="搜索文章" />
                <i class="fas fa-search gt-c-content-color-first" style="position: absolute; top: 9px; left: 10px;"></i>
            </form>
        </div>
    </div>
</nav>

    <div class="post-container">
        <div class="post-detail gt-bg-theme-color-second">
            <article class="gt-post-content">
                <h2 class="post-title">
                    Pytorch_Advance
                </h2>
                <div class="post-info">
                    <time class="post-time gt-c-content-color-first">
                        · 2023-06-27 ·
                    </time>
                    
                </div>
                <div class="post-content">
                    <h1 id="deterministic-的问题">Deterministic 的问题</h1>
<blockquote>
<p>其实没必要严格要求 deterministic，从实际角度出发，只要每次跑出来结果差距都不大就行了：https://pytorch.org/docs/stable/notes/randomness.html</p>
</blockquote>
<ul>
<li><code>torch.backends.cudnn.deterministic=True</code> 是能让卷积操作 deterministic，其他操作如 <code>torch.nn.MaxPool3d</code> 基本没办法确保deterministic： https://stackoverflow.com/a/66647424</li>
<li>初始化也不行：https://github.com/pytorch/pytorch/issues/19013</li>
<li>Dataloader 的 worker 数量不同也会使得采样结果 non-deterministic: https://pytorch.org/docs/stable/data.html#data-loading-randomness</li>
</ul>
<br>
<br>
<h1 id="pytorch多gpu并行训练">pytorch多gpu并行训练</h1>
<blockquote>
<p>https://zhuanlan.zhihu.com/p/105755472<br>
https://zhuanlan.zhihu.com/p/86441879<br>
https://zhuanlan.zhihu.com/p/95700549<br>
https://zhuanlan.zhihu.com/p/68717029</p>
</blockquote>
<h2 id="high-level-介绍-dp-和-ddp">High-level 介绍 DP 和 DDP</h2>
<blockquote>
<p><a href="https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255">Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU &amp; Distributed setups<br>
(里面关于DP的图有错误，DP中从loss计算梯度仍是并行的)</a></p>
</blockquote>
<ul>
<li>DP:
<ul>
<li>实现是单进程多线程，只能用于单机多卡；并且用的 parameter server 构架，会导致GPU之间通信是瓶颈；好处是代码改动很少</li>
<li>并且其中一个 GPU 需要计算 loss 并整合梯度，会导致负载不均衡。具体，并行训练分以下四步<br>
（其中下图左下角的两步画错了，应该是GPU-1计算并分发loss，而不是分发梯度）：<br>
1：把每张卡的输出聚集到 GPU0<br>
2：只在 GPU0 在算 loss<br>
3：把 loss scatter 到其他 GPU 上<br>
4：每个 GPU 算各自的梯度，再把梯度汇总到 GPU0 上  <p align="center" >
      <img src="./pictures/dp_torch.png" width=800>
  </p>
</li>
</ul>
</li>
<li>可以看到 loss 的计算始终在一个 GPU 上，所以会有负载不均衡，所以有类似 PyTorch Encoding 的包，使得 loss 的计算也能并行。也即每张卡算自己的 loss，然后再算自己的梯度，最后只需将梯度聚集就可以
<ul>
<li>但这种方法不适用于对比学习，因为对比学习 loss 计算本身就必须知道一个batch内所有样本的 output</li>
<li>这时普通的 DDP 也不适用，需要用 DDP2 来计算 NCE loss，见 https://pytorch-lightning.readthedocs.io/en/stable/advanced/multi_gpu.html#distributed-data-parallel-2</li>
</ul>
</li>
<li>DDP：实现是多进程，可用于单机多卡或多机多卡，用的 ring-all-reduce 构架
<ul>
<li>加速主要来源于3点：多进程，ring-all-reduce，负载均衡；可以见到没有聚合output在一张卡上计算梯度的步骤了，而是根据每张卡上的loss直接反传  <p align="center" >
      <img src="./pictures/ddp_torch.png" width=800>
  </p>
</li>
</ul>
</li>
</ul>
<h2 id="ddp-具体用法">DDP 具体用法</h2>
<ul>
<li>
<p>三种方式<br>
https://zhuanlan.zhihu.com/p/358974461<br>
https://zhuanlan.zhihu.com/p/187610959</p>
<ul>
<li>每个进程负责一个GPU，推荐</li>
<li>每个进程多GPU，但每个进程都是 DP，但 python 的 PIL 会造成 CPU bound</li>
<li>每个进程调用多个GPU，主要用于 model 太大 batchsize=1 都放不下的情况</li>
</ul>
<pre><code class="language-python">import os
import torch
import torchvision
import torch.distributed as dist
import torch.utils.data.distributed
from torchvision import transforms
from torch.multiprocessing import Process

os.environ['MASTER_ADDR'] = 'localhost'     # New added
os.environ['MASTER_PORT'] = '12355'         # New added


def main(rank):

    dist.init_process_group(&quot;ncll&quot;, rank=rank, world_size=3)    # New added
    torch.cuda.set_device(rank)                                 # New added

    trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])
    data_set = torchvision.datasets.MNIST(&quot;./&quot;, train=True, transform=trans, target_transform=None, download=True)

    train_sampler = torch.utils.data.distributed.DistributedSampler(data_set)   # New added
    data_loader_train = torch.utils.data.DataLoader(dataset=data_set, batch_size=256, sampler=train_sampler)    # use train_sampler to split the original batch size

    net = torchvision.models.resnet101(num_classes=10)
    net.conv1 = torch.nn.Conv1d(1, 64, (7, 7), (2, 2), (3, 3), bias=False)
    net = net.cuda()

    net = torch.nn.parallel.DistributedDataParallel(net, device_ids=[rank]) # New added
    criterion = torch.nn.CrossEntropyLoss()
    opt = torch.optim.Adam(net.parameters(), lr=0.001)
    for epoch in range(10):
        for i, data in enumerate(data_loader_train):
            images, labels = data
            images, labels = images.cuda(), labels.cuda()
            opt.zero_grad()
            outputs = net(images)
            loss = criterion(outputs, labels)
            loss.backward()
            opt.step()
            if i % 10 == 0:
                print(&quot;loss: {}&quot;.format(loss.item()))
    if rank == 0:       # only the main process saves the model 
        torch.save(net, &quot;my_net.pth&quot;)   


if __name__ == &quot;__main__&quot;:
    size = 3
    processes = []
    for rank in range(size):
        p = Process(target=main, args=(rank,))
        p.start()
        processes.append(p)
    for p in processes:
        p.join()

    # or use spawn
    # mp.spawn(main, args=(size,), nprocs=size, join=True)
</code></pre>
</li>
<li>
<p>torch.distributed.init_process_group() 默认 <code>env://</code> 的初始方法，也可以使用 tcp 和 file</p>
</li>
<li>
<p>多机分布式启动方式： 用 torch.distributed.launch。假设一共有两台机器（节点1和节点2），每个节点上有8张卡，节点1的IP地址为192.168.1.1，占用的端口12355（端口可以更换），启动的方式如下：</p>
<blockquote>
<p>其中 torch.distributed.launch 将会被 torchrun 代替：https://pytorch.org/docs/stable/elastic/run.html#launcher-api</p>
</blockquote>
<pre><code class="language-bash"># 节点1
python -m torch.distributed.launch --nproc_per_node=8
        --nnodes=2 --node_rank=0 --master_addr=&quot;192.168.1.1&quot;
        --master_port=12355 MNIST.py
# 节点2
python -m torch.distributed.launch --nproc_per_node=8
        --nnodes=2 --node_rank=1 --master_addr=&quot;192.168.1.1&quot;
        --master_port=12355 MNIST.py
</code></pre>
</li>
<li>
<p>按模块启动：<code>python -m torch.distributed.launch main.py</code></p>
<blockquote>
<p>https://www.cnblogs.com/xueweihan/p/5118222.html</p>
</blockquote>
<ul>
<li>直接启动 <code>python xxx.py</code> 是把 <code>xxx.py</code> 文件所在的目录放到了sys.path属性中</li>
<li>按模块启动 <code>python -m xxx.py</code> 是把你输入命令的目录（也就是当前路径），放到了sys.path属性中</li>
</ul>
</li>
</ul>
<br>
<br>
<h1 id="多gpu多进程模拟联邦学习进程初始化报错">多GPU多进程模拟联邦学习，进程初始化报错</h1>
<blockquote>
<p>https://discuss.pytorch.org/t/understanding-minimum-example-for-torch-multiprocessing/101010</p>
</blockquote>
<h2 id="背景">背景</h2>
<ul>
<li>由于要调用 CUDA，多进程用的是 <code>spawn</code> 而不是 <code>fork</code></li>
<li>多进程之间是要共享一个 list of tensors，但不同进程用的是 list 里面不同的 slices</li>
</ul>
<p>一个错误复现的代码如下：</p>
<pre><code class="language-python">import torch, os
import torch.distributed as dist
import torch.multiprocessing as mp

def run(rank, size):
    &quot;&quot;&quot; Distributed function to be implemented later. &quot;&quot;&quot;
    print(f'rank {rank} of {size}')

def init_process(rank, size, data, fn, backend='gloo'):
    &quot;&quot;&quot; Initialize the distributed environment. &quot;&quot;&quot;
    os.environ['MASTER_ADDR'] = '127.0.0.1'
    os.environ['MASTER_PORT'] = '29500'
    dist.init_process_group(backend, rank=rank, world_size=size)
    fn(rank, size)

def processes(data):
    size = len(data)
    processes = []
    for rank in range(size):
        p = mp.Process(target=init_process, args=(rank, size, data, run))
        p.start()
        processes.append(p)
    for p in processes:
        p.join()

if __name__ == &quot;__main__&quot;:
    mp.set_start_method(&quot;spawn&quot;)
    size_vector = 133
    part = int(size_vector/8)
    indices = torch.arange(size_vector)
    split_data = torch.split(indices, part)
    print(split_data)
    processes(split_data)
</code></pre>
<h2 id="解决方案">解决方案</h2>
<ul>
<li>以上代码在 Pytorch1.9 的Linux版本下会报错：<code> ValueError: bad value(s) in fds_to_keep</code>，在windows下则正常运行</li>
<li>解决方案有两种：
<ul>
<li>将 <code>mp.Process()</code> 那行改成：<code>data_i = data[rank]; p = mp.Process(target=init_process, args=(rank, size, data_i, run))</code></li>
<li>在 <code>mp.Process()</code> 那行之前加：<code>data = [i.clone() for i in data]</code>，相当于将 data 的每个元素重新在内存里面复制了一遍</li>
</ul>
</li>
</ul>
<h2 id="原因分析">原因分析</h2>
<blockquote>
<p>You cannot pass a tensor to the mp.Process that has data shared with other processes</p>
</blockquote>
<ul>
<li>报错的的原因应该是来源于 Pytorch 内部的内存管理机制，tensor 和 tensor 之间是自动共享内存的。试过 <code>copy.deepcopy(data)</code> 不起作用，必须要用 <code>.clone()</code></li>
<li>为什么 windows 不报错而 linux 报错，可能和两个平台的多进程实现机制有关，windows没有fork，所以为新进程强行开了新的存储空间</li>
</ul>
<br>
<br>
<h1 id="pytorch-internals">Pytorch Internals</h1>
<h2 id="folders">Folders</h2>
<ul>
<li>
<p><code>torch/</code>：包含导入和使用的实际Python模块。Python代码，很容易上手调试。</p>
</li>
<li>
<p><code>torch/csrc/</code>：它实现了在Python和C++之间进行转换的绑定代码，以及一些非常重要的PyTorch功能，如autograd引擎和JIT编译器。它还包含C++前台代码。</p>
<ul>
<li><code>torch._C</code> 模块在 <code>torch/csrc/Module.cpp</code> 中定义。这个模块被称为是一个扩展模块（一个用C实现的Python模块），它允许我们定义新的内建对象类型（例如：Tensor）并调用 C/C++ 函数。</li>
</ul>
</li>
<li>
<p><code>aten/</code>：“A Tensor Library”的缩写（由Zachary DeVito创造），是一个实现Tensors操作的C++库。存放一些内核代码存在的地方，尽量不要在那里花太多时间。</p>
</li>
<li>
<p><code>c10/</code>：这是一个双关语。C代表Caffe，10既是二级制的2 (Caffe2)，也是十进制的10（英文Ten，同时也是Tensor的前半部分）。包含PyTorch的核心抽象，包括Tensor和Storage数据结构的实际实现。</p>
</li>
</ul>
<br>
<br>
<h1 id="pytorch-features">Pytorch Features</h1>
<blockquote>
<p>https://speakerdeck.com/perone/pytorch-under-the-hood?slide=21</p>
</blockquote>
<h2 id="tensors">Tensors</h2>
<ul>
<li>
<p>Although PyTorch has an elegant python first design, all PyTorch heavy work is actually implemented in C++. The integration of C++ code is usually done using what is called an <strong>extension</strong>.</p>
</li>
<li>
<p>zero-copy tensors</p>
<pre><code class="language-python"># a copy is made
np_array = np.ones((1,2))
torch_array = torch.tensor(np_array)    # This make a copy
torch_array.add_(1.0)   # underline after an operation means an in-place operation
print(np_array)     # array([[1., 1.]])

# zero-copy
np_array = np.ones((1,2))
torch_array = torch.from_numpy(np_array)    # This make a copy
torch_array.add_(1.0) # or torch_array += 1.0 (in place operation)
print(np_array)     # array([[2., 2.]])

# zero-copy
np_array = np.ones((1,2))
torch_array = torch.from_numpy(np_array)    # This make a copy
torch_array = torch_array + 1.0     # not an in-place operatio on torch_array 
print(np_array)     # array([[1., 1.]])
</code></pre>
<p>The tensor <strong>FloatTensor</strong> did a copy of the <strong>numpy array data pointer</strong> instead of the contents. The reference is kept safe by Python reference counting mechanism.</p>
</li>
<li>
<p>The abstraction responsible for holding the data isn't actually the Tensor, but the Storage. We can have multiple tensors sharing the same storage, but with different interpretations, also called views, but without duplicating memory.</p>
<pre><code class="language-python">t_a = torch.ones((2,2))
t_b = t_a.view(4)
t_a_data = t_a.storage().data_ptr()
t_b_data = t_b.storage().data_ptr()
t_a_data == t_b_data

# True
</code></pre>
</li>
</ul>
<h2 id="jit-just-in-time-compiler">JIT: just-in-time compiler</h2>
<blockquote>
<p>https://zhuanlan.zhihu.com/p/52154049<br>
https://zhpmatrix.github.io/2019/03/01/c++-with-pytorch/</p>
</blockquote>
<p>早期的PyTorch只有Python前端，但对于工业界的实际部署问题，Python语言太慢，可移植性和适用性根本无法和C++相比。当时的一个想法是，PyTorch训练模型，然后前向推断时将结构和参数灌入到C++代码中，这估计也是早些年的一些做法。但是调研之后，将PyTorch的C++后端拉出来并不容易，而且如果从C++原生代码来写起，工作量也很大。因此，希望有一个C++前端方便做推断部署。</p>
<p>千呼万唤始出来。PyTorch1.0发布了，这样业界部署的工作流程可以变成这样：</p>
<blockquote>
<p>论文发布-&gt;PyTorch开源代码(或者自己实现)-&gt;训练模型-&gt;导出模型-&gt;载入模型(C++/Python/其他框架/其他硬件平台)</p>
</blockquote>
<p>PyTorch1.0后，可以通过两种方式，分别是Tracing和Script，将一个Python代码转化为TorchScript代码，继而导出相应的模型可以继续被优化，同时被C++所调用，最终实现对生产环境下的支持（考虑到多线程执行和性能原因，一般Python代码并不适合做部署）</p>
<ul>
<li>
<p>Tracing<br>
Tracing方式对于含有if和for-loop的场景失效，需要用script方式</p>
</li>
<li>
<p>Script<br>
https://zhpmatrix.github.io/2019/03/09/torch-jit-pytorch/</p>
</li>
</ul>
<h2 id="dataloader-加速">Dataloader 加速</h2>
<ul>
<li>
<p>仅从使用者的角度考虑,DataLoader做了下面的事情：</p>
<ul>
<li>开启多个子进程worker</li>
<li>每个 worker 通过主进程获得自己需要采集的idx。idx的顺序由采样器（sampler）或 shuffle 得到。每个 worker 开始采集一个batch的数据。因此增大 num_workers 的数量，内存（不是显存）占用也会增加。因为每个 worker 都需要缓存一个 batch 的数据</li>
<li>第一个 worker 数据采集完成后，会卡在这里，等着主进程取走数据。主进程处理完这个 batch 之后，这个 worker 开始采集下一个 batch</li>
<li>主进程采集完最后一个 worker 的batch。此时需要回去采集第一个 worker 产生的第二个 batch。如果第一个 worker 此时没有采集完，主线程会卡在这里等（这也是为什么在数据加载比较耗时的情况下，每隔 num_workers 个 batch，主进程都会在这里卡一下）</li>
</ul>
</li>
<li>
<p>Dataloader 数据装载阻塞的问题: https://zhuanlan.zhihu.com/p/91521705</p>
  <p align="center" >
      <img src="pictures/dataloader.jpg", width='800'>
  </p>
<ul>
<li>Pytorch Dataloader 的实现是多进程</li>
<li>一个 worker 独立的处理一个 batch，而不是多个 worker 同时处理一个 batch</li>
<li>dataloader <strong>不是</strong> 等所有worker数据取完才进行下一批次的数据读取，worker 之间并没有同步</li>
<li>输出的数据保持顺序性：主线程（进行front/back propagation）按照<code>idx=0, 1, 2, 3...</code>依次处理 worker 产生的 batch</li>
<li>worker 会等待主进程处理完（主要即GPU time）上个 batch，才采样下一个 batch</li>
</ul>
</li>
<li>
<p>用 GPU 来完成 dataloader 中的 transform:<br>
https://zhuanlan.zhihu.com/p/77633542<br>
https://github.com/pytorch/pytorch/issues/31359</p>
</li>
<li>
<p>进一步加速：</p>
<blockquote>
<p>https://www.cnblogs.com/pprp/p/14199865.html</p>
</blockquote>
<ul>
<li>Prefetch next batch / 新开的cuda stream拷贝tensor到gpu：https://zhuanlan.zhihu.com/p/97190313</li>
<li>生产者消费者模型：https://blog.csdn.net/winycg/article/details/92443146</li>
</ul>
</li>
</ul>
<h2 id="apex">APEX</h2>

                </div>
            </article>
        </div>

        
            <div class="next-post">
                <div class="next gt-c-content-color-first">下一篇</div>
                <a href="https://kimokcheon.github.io/post/pytorch/" class="post-title gt-a-link">
                    Pytorch
                </a>
            </div>
        

        

        

        
            <script src='https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js'></script>

<style>
	div#vcomments{
		width:100%;
		max-width: 1000px;
		padding: 2.5%
	}
</style>


	<div id="vcomments"></div>

<script>
	new Valine({
		el: '#vcomments',
		appId: '',
		appKey: '',
		avatar: '',
		pageSize: 5,
		recordIp: false,
		placeholder: 'Just Go Go',
		visitor: false,
	});
</script>

        

        <div class="site-footer gt-c-content-color-first">
    <div class="slogan gt-c-content-color-first">他们都是萤火，聚在一起就成了太阳</div>
    <div class="social-container">
        
            
                <a href="https://github.com/Kimokcheon" target="_blank">
                    <i class="fab fa-github gt-c-content-color-first"></i>
                </a>
            
        
            
        
            
        
            
                <a href="https://www.zhihu.com/people/deng-yu-chuan-4" target="_blank">
                    <i class="fab fa-zhihu gt-c-content-color-first"></i>
                </a>
            
        
            
        
            
        
    </div>
    <div class="footer-info">
        Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a>
    </div>
    <div>
        Theme by <a href="https://imhanjie.com/" target="_blank">imhanjie</a>, Powered by <a
                href="https://github.com/getgridea/gridea" target="_blank">Gridea | <a href="https://kimokcheon.github.io//atom.xml" target="_blank">RSS</a></a>
    </div>
</div>

<script>
  hljs.initHighlightingOnLoad()
</script>

    </div>
</div>
</body>
</html>
