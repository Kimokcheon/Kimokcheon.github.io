<html>
<head>
    <meta charset="utf-8"/>
<meta name="description" content=""/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>

<title>Pytorch | Yuchuan&#39;s Blog</title>

<link rel="shortcut icon" href="https://kimokcheon.github.io//favicon.ico?v=1695778991029">

<link href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://kimokcheon.github.io//styles/main.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.0/dist/css/bootstrap.min.css">

<script src="https://cdn.jsdelivr.net/npm/@highlightjs/cdn-assets/highlight.min.js"></script>
<script src="https://cdn.bootcss.com/highlight.js/9.15.10/languages/dockerfile.min.js"></script>
<script src="https://cdn.bootcss.com/highlight.js/9.15.10/languages/dart.min.js"></script>

<script src="https://cdn.jsdelivr.net/npm/moment@2.27.0/moment.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.0/dist/js/bootstrap.min.js"></script>
<!-- DEMO JS -->
<!--<script src="media/scripts/index.js"></script>-->



    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.css">
</head>
<body>
<div class="main gt-bg-theme-color-first">
    <nav class="navbar navbar-expand-lg">
    <div class="navbar-brand">
        <img class="user-avatar" src="/images/avatar.png" alt="头像">
        <div class="site-name gt-c-content-color-first">
            Yuchuan&#39;s Blog
        </div>
    </div>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <i class="fas fa-bars gt-c-content-color-first" style="font-size: 18px"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <div class="navbar-nav mr-auto" style="text-align: center">
            
                <div class="nav-item">
                    
                        <a href="/" class="menu gt-a-link">
                            首页
                        </a>
                    
                </div>
            
                <div class="nav-item">
                    
                        <a href="/archives" class="menu gt-a-link">
                            归档
                        </a>
                    
                </div>
            
                <div class="nav-item">
                    
                        <a href="/tags" class="menu gt-a-link">
                            标签
                        </a>
                    
                </div>
            
                <div class="nav-item">
                    
                        <a href="https://kimokcheon.github.io/post/about-me" class="menu gt-a-link">
                            关于
                        </a>
                    
                </div>
            
        </div>
        <div style="text-align: center">
            <form id="gridea-search-form" style="position: relative" data-update="1695778991029" action="/search/index.html">
                <input class="search-input" autocomplete="off" spellcheck="false" name="q" placeholder="搜索文章" />
                <i class="fas fa-search gt-c-content-color-first" style="position: absolute; top: 9px; left: 10px;"></i>
            </form>
        </div>
    </div>
</nav>

    <div class="post-container">
        <div class="post-detail gt-bg-theme-color-second">
            <article class="gt-post-content">
                <h2 class="post-title">
                    Pytorch
                </h2>
                <div class="post-info">
                    <time class="post-time gt-c-content-color-first">
                        · 2023-06-27 ·
                    </time>
                    
                </div>
                <div class="post-content">
                    <h1 id="1-pytorch的安装">1. Pytorch的安装</h1>
<blockquote>
<p>可考虑是否在虚拟环境中安装， 安装步骤见 https://pytorch.org/</p>
</blockquote>
<h3 id="11-cudacudnn-的区别">1.1 CUDA，cuDNN 的区别？</h3>
<blockquote>
<p>见 <a href="../Hardware_Related/CUDA_Installation">CUDA Installation</a></p>
</blockquote>
<ul>
<li>CUDA is a general purpose parallel computing <strong>PLATFORM</strong> and programming model that leverages the parallel compute engine in NVIDIA GPUs in a more efficient way on a CPU</li>
<li>cuDNN(CUDA Deep Neural Network library) is a <strong>LIBRARY</strong></li>
</ul>
<hr>
<p><br><br></p>
<h1 id="2-pytorch-的使用">2. Pytorch 的使用</h1>
<h2 id="21-常用命令">2.1 常用命令</h2>
<h3 id="211-torchnnsequential-和-torchnnmodulelist">2.1.1 torch.nn.Sequential() 和 torch.nn.ModuleList()</h3>
<blockquote>
<p>见 https://zhuanlan.zhihu.com/p/64990232</p>
</blockquote>
<ul>
<li>ModuleList 就是一个储存各种模块的 list，这些模块之间没有联系，没有实现 forward 功能。相比于普通的 Python list，ModuleList 可以把添加到其中的模块和参数自动注册到网络上。</li>
<li>Sequential 内的模块需要按照顺序排列，要保证相邻层的输入输出大小相匹配，内部 forward 功能已经实现，可以使代码更加整洁。</li>
</ul>
<h3 id="212-torchnn-和-torchnnfunctional-的区别">2.1.2 torch.nn 和 torch.nn.functional 的区别</h3>
<blockquote>
<p>torch.nn API: https://pytorch.org/docs/stable/nn.html</p>
</blockquote>
<ul>
<li>torch.nn 是里面包含的是，torch.nn.functional 里面包含的是函数。</li>
<li>如果我们只保留nn.functional下的函数的话，在训练或者使用时，我们就要手动去维护weight, bias, stride这些中间量的值，这显然是给用户带来了不便。</li>
<li>而如果我们只保留nn下的类的话，其实就牺牲了一部分灵活性，因为做一些简单的计算都需要创造一个类，这也与PyTorch的风格不符。
<blockquote>
<p>见 https://www.zhihu.com/question/66782101/answer/246341271</p>
</blockquote>
</li>
</ul>
<h3 id="213-torchno_grad-torchset_grad_enabled-torchenable_grad-和-modeleval">2.1.3  torch.no_grad(),  torch.set_grad_enabled(), torch.enable_grad() 和 model.eval()</h3>
<blockquote>
<p>见 https://www.cnblogs.com/guoyaohua/p/8724433.html</p>
</blockquote>
<ul>
<li>
<p><code>model.eval()</code>: changes the forward() behaviour of the module it is called upon. It disables certain layers exclusive for training stage. BN层一般放在conv层后面，激活函数之前；Dropout对于conv层和FC层都可以适用</p>
<ul>
<li>在train模式下，dropout网络层会按照设定的参数p设置保留激活单元的概率（保留概率=p); batchnorm层会继续计算数据的mean和var等参数并更新</li>
<li>在val模式下，dropout层会让所有的激活单元都通过；而batchnorm层会停止计算和更新mean和var，用从所有训练实例中获得的统计量来代替Mini-Batch里面m个训练实例获得的mean和var的统计量</li>
</ul>
</li>
<li>
<p><code>torch.no_grad()</code> or <code>torch.set_grad_enabled(False)</code>: Disable the gradient computation. In this mode, the result of every computation will have <code>requires_grad=False</code>, even when the inputs have <code>requires_grad=True</code>.</p>
<pre><code class="language-python"># There is no different bewteen: 

with torch.no_grad():
    &lt;code&gt;
    
# and

torch.set_grad_enabled(False)
    &lt;code&gt;
torch.set_grad_enabled(True)   

# 只是torch.set_grad_enabled()可以选择是开还是关梯度计算，
# torch.no_grad()只能选择关
</code></pre>
</li>
<li>
<p><code>torch.enable_grad()</code>: Enables gradient calculation, if it has been disabled via <code>no_grad()</code> or <code>set_grad_enabled(False)</code>.</p>
<pre><code class="language-python">x = torch.tensor([1], requires_grad=True)
with torch.no_grad():
    with torch.enable_grad():
    y = x * 2
y.requires_grad
# 输出 True
</code></pre>
</li>
<li>
<p><code>model.eval()</code> 和 <code>torch.no_grad()</code> 可以一起使用：</p>
<pre><code class="language-python">model = CNN()
for e in num_epochs:
    # do training
    model.train()

# evaluate model:
model = model.eval()
with torch.set_grad_enabled(False): 
    logits, probas = model(testset_features)
</code></pre>
</li>
</ul>
<h3 id="214-modelzero_grad-和-optimizerzero_grad">2.1.4 model.zero_grad() 和 optimizer.zero_grad()</h3>
<blockquote>
<p>https://pytorch.org/tutorials/beginner/former_torchies/autograd_tutorial.html</p>
</blockquote>
<ul>
<li>optimizer.zero_grad() 有什么用 ?<br>
一般的训练方式是进来一个batch更新一次梯度，所以每次计算梯度前都需要用 optimizer.zero_grad() 手动将梯度清零。如果不手动清零，pytorch会自动对梯度进行累加。
<ul>
<li>梯度累加可以模拟更大的batch size，在内存不够大的时候，是一种用更大batch size训练的trick，见 https://www.zhihu.com/question/303070254/answer/573037166</li>
<li>梯度累加可以减少multi-task时的内存消耗问题。因为当调用了.backward()后，computation graph就从内存释放了。这样进行multi-task时，在任意时刻，在内存中最少只存储一个graph。 见 https://www.zhihu.com/question/303070254/answer/608153308<pre><code class="language-python">for idx, data in enumerate(train_loader):
  xs, ys = data
  
  optmizer.zero_grad()
  # 计算d(l1)/d(x)
  pred1 = model1(xs) #生成graph1
  loss = loss_fn1(pred1, ys)
  loss.backward()  #释放graph1

  # 计算d(l2)/d(x)
  pred2 = model2(xs) #生成graph2
  loss2 = loss_fn2(pred2, ys)
  loss.backward()  #释放graph2

  # 使用d(l1)/d(x)+d(l2)/d(x)进行优化
  optmizer.step()
</code></pre>
</li>
</ul>
</li>
<li>model.zero_grad() 和 optimizer.zero_grad() 的区别<br>
当<code>optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)</code>时，二者等效，其中SGD也可以换成其他优化器例如Adam。当一个model中用了多个optimizer时，model.zero_grad() 是将所有梯度清零，optimizer.zero_grad() 是清零一个optimizer</li>
</ul>
<h3 id="215-learning-rate-decay">2.1.5 learning rate decay</h3>
<blockquote>
<p>https://www.cnblogs.com/wanghui-garcia/p/10895397.html</p>
</blockquote>
<p>一般会用到<code>torch.optim.lr_scheduler.LambdaLR</code>， <code>torch.optim.lr_scheduler.StepLR</code>， <code>torch.optim.lr_scheduler.MultiStepLR</code>，使用格式为：</p>
<pre><code class="language-python">import torch.optim.lr_scheduler.StepLR
scheduler = StepLR(optimizer, step_size=30, gamma=0.1)
for epoch in range(100):
    scheduler.step()
    optimier.step()
    optimizer.zero_grad()
    (other training steps...)
    (other validate steps...)
</code></pre>
<p>查看 <code>learning rate</code>: <code>print(optimizer.param_groups[0]['lr'])</code></p>
<h3 id="216-hook">2.1.6 hook</h3>
<blockquote>
<p>https://zhuanlan.zhihu.com/p/75054200</p>
</blockquote>
<p>pytorch 中，对于中间变量（由别的变量计算得到的变量）/ Module，一旦完成了反向传播，它就会被释放掉以节约内存。利用hook，我们不必改变网络输入输出的结构，就可方便地获取、改变网络中间层变量的梯度</p>
<ul>
<li>
<p>Hook for Tensors：<code>z.register_hook(hook_fn)</code><br>
使用方式： <code>y.register_hook(fn)</code>，其中自定义函数<code>fn(grad)</code>返回Tensor或没有返回值</p>
<pre><code class="language-python">def save_grad():
    def hook(grad):
        print(grad)
    return hook

# register gradient hook for tensor y
if y.requires_grad == True:
    y.register_hook(save_grad())
</code></pre>
</li>
<li>
<p>Hook for Modules：</p>
<ul>
<li><code>module.register_forward_hook(hook_fn)</code>：获取前向传播时，module的输入输出</li>
<li><code>module.register_backward_hook(hook_fn)</code>：执行反向传播时，module前后的梯度</li>
<li><code>module.register_forward_pre_hook(hook_fn)</code>：获取前向传播执行前的hook，在 <code>torch.nn.utils.prune</code> 会用到，见 https://pytorch.org/tutorials/intermediate/pruning_tutorial.html</li>
</ul>
<p>使用方法：编写 <code>hook_fn</code> 函数（包含打印或保存等操作）-&gt; 注册hook -&gt; 执行前向/方向传播</p>
</li>
</ul>
<h3 id="217-nnmodule">2.1.7 nn.Module</h3>
<ul>
<li>
<p>一个Net，也就是继承自 <code>nn.Module</code> 的类，当实例化后，本质上就是维护了以下8个字典(OrderedDict)，在上面和本小节都有介绍:</p>
<blockquote>
<p>https://www.jianshu.com/p/a4c745b6ea9b</p>
</blockquote>
<pre><code>_parameters
_buffers
_backward_hooks
_forward_hooks
_forward_pre_hooks
_state_dict_hooks
_load_state_dict_pre_hooks
_modules
</code></pre>
<ul>
<li><code>model.modules()</code>：返回Generator，广度优先遍历模型所有子层<pre><code class="language-python">for x in model.modules()：
</code></pre>
</li>
<li><code>model.named_modules()</code>：返回Generator，比 <code>model.modules()</code> 多返回了每个module的名字<pre><code class="language-python">for name, layer in model.named_modules():
    if isinstance(layer, nn.Conv2d):
</code></pre>
</li>
<li><code>model.children()</code>：返回Generator，只遍历model的子层（子层的子层不遍历了）</li>
<li><code>model.named_children()</code>：返回Generator， 比<code>model.children()</code> 多返回了每个child的名字</li>
<li><code>model.parameters()</code>：返回Generator，迭代地返回所有参数，一般用于给optimizer传递参数</li>
<li><code>model.named_parameters()</code>：返回Generator，比<code>model.parameters()</code> 多返回了每个参数的名字，weights和bias也加以了区分</li>
<li><code>model.state_dict()</code>：返回OrderDict，一般用于模型保存<pre><code class="language-python">for k,v in model.state_dict():
</code></pre>
</li>
<li><code>model.buffers()</code>：返回OrderDict。反向传播不需要被optimizer更新的参数（和parameter正好相反），称之为buffer，用于存一些不变的模型参数（例如存BN的mean，存pruning时的mask）<pre><code class="language-python">bn = nn.BatchNorm1d(2)
input = torch.tensor(torch.rand(3, 2), requires_grad=True)
output = bn(input)
print(bn._buffers)

# 输出 OrderedDict([('running_mean', tensor([0.0525, 0.0584])), ('running_var', tensor([0.9177, 0.9140])), ('num_batches_tracked', tensor(1))])
</code></pre>
</li>
</ul>
<blockquote>
<p>注意： <code>len([x for x in model.modules()])</code> 会比 <code>len([[x for x in model.parameters()])</code> 大，因为前者遍历了 a hierarchy of model（包含中间节点），后者只遍历了 model graph 的叶节点</p>
</blockquote>
</li>
<li>
<p>例子</p>
<ul>
<li>
<p>打印网络每层名字，大小，是否需要梯度</p>
<pre><code class="language-python">for name, param in model.named_parameters():
    print(name, ' ', param.size(), param.requires_grad)
    print(type(param), type(param.detach()))  # &lt;class 'torch.nn.parameter.Parameter'&gt; &lt;class 'torch.Tensor'&gt;
    # torch.nn.parameter.Parameter 是 torch.Tensor 的子类，用了 .detach() 梯度就不反传了  
</code></pre>
</li>
<li>
<p>初始化网络：<br>
https://blog.csdn.net/daydayjump/article/details/80899029</p>
</li>
</ul>
</li>
</ul>
<h3 id="218-其他">2.1.8 其他</h3>
<ul>
<li>
<p>torchvision 由以下四部分组成：<br>
torchvision.datasets， torchvision.models， torchvision.transforms， torchvision.utils</p>
<blockquote>
<p>见 https://pytorch.org/docs/master/torchvision/transforms.html?highlight=torchvision%20transforms</p>
</blockquote>
<ul>
<li>torchvision.transforms 包含很多类，其中 torchvision.transforms.Compose() 可以把多个步骤合在一起<br>
例如 torchvision.transforms.Compose([transforms.CenterCrop(10), transforms.ToTensor()])</li>
</ul>
</li>
<li>
<p>In PyTorch, every method that ends with an underscore (_) makes changes in-place, meaning, they will modify the underlying variable.</p>
</li>
</ul>
<br>
<h2 id="22-tensor相关">2.2 Tensor相关</h2>
<h3 id="221-一个例子">2.2.1 一个例子</h3>
<pre><code class="language-python">import torch

x = torch.Tensor([[1.,2.,3.],[4.,5.,6.]])
x.requires_grad = True
y = x + 1
z = y * y
out = z.mean()
loss = 20 - out
loss.backward()

print(x.data, '\n', x.dtype, '\n', x.device, 
      '\n', x.grad, '\n', x.grad_fn, '\n', x.requires_grad, '\n')
print(x)
print(y)
print(z)
print(out)
</code></pre>
<p>运行得到:</p>
<pre><code>tensor([[1., 2., 3.],
        [4., 5., 6.]]) 
torch.float32 
cpu 
tensor([[-0.6667, -1.0000, -1.3333],
      [-1.6667, -2.0000, -2.3333]]) 
None 
True 

tensor([[1., 2., 3.],
        [4., 5., 6.]], requires_grad=True)
tensor([[2., 3., 4.],
        [5., 6., 7.]], grad_fn=&lt;AddBackward0&gt;)
tensor([[ 4.,  9., 16.],
        [25., 36., 49.]], grad_fn=&lt;MulBackward0&gt;)
tensor(23.1667, grad_fn=&lt;MeanBackward1&gt;)
</code></pre>
<ul>
<li>x是一个tensor， x的核心部分 x.data 可以理解成一个n-dimensional array。 此外，tensor还有其他几个属性： <code>x.dtpe, x.device, x.grad, x.grad_fn</code>等， 其中：
<ul>
<li>x.grad 是求得的梯度</li>
<li>x.requires_grad 表示该变量是否需要autograd</li>
<li>y.grad_fn 记录了该变量求导应该用的function。 例如y由加法得到， y.grad_fn = &lt;AddBackward0&gt;; z由乘法得到， y.grad_fn = &lt;MulBackward0&gt;</li>
</ul>
</li>
</ul>
<h3 id="222-tensor的操作">2.2.2 tensor的操作</h3>
<p>tensor 能像 numpy array 一样进行索引</p>
<ul>
<li>
<p>max 操作<br>
<code>.max(k)</code>表示求第k维的最大值，对于二维tensor，求列最大 k = 0，行最大 k = 1</p>
<pre><code class="language-python">import torch
a = torch.tensor([[1, 2], [3, 4], [5, 6]])
print(a.max(1))     # 默认keepdim为false
print(a.max(1)[0])  # 最大值
print(a.max(1)[1])  # 最大值对应的index
print(a.max(1, keepdim = True))
</code></pre>
<p>输出</p>
<pre><code>torch.return_types.max(values=tensor([2, 4, 6]), indices=tensor([1, 1, 1])) 
tensor([2, 4, 6]) 
tensor([1, 1, 1])
torch.return_types.max(values=tensor([[2], [4], [6]]), indices=tensor([[1], [1], [1]])) 
</code></pre>
</li>
<li>
<p>矩阵操作</p>
<table>
<thead>
<tr>
<th style="text-align:left">用途</th>
<th style="text-align:left">命令</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">创建随机数矩阵</td>
<td style="text-align:left">x = torch.rand(5, 3)</td>
</tr>
<tr>
<td style="text-align:left">创建正态分布随机数矩阵</td>
<td style="text-align:left">x = torch.randn(2,4)</td>
</tr>
<tr>
<td style="text-align:left">创建空矩阵</td>
<td style="text-align:left">x = torch.empty(5, 3)</td>
</tr>
<tr>
<td style="text-align:left">创建零矩阵并指定类型</td>
<td style="text-align:left">x = torch.zeros(5, 3, dtype=torch.long)</td>
</tr>
<tr>
<td style="text-align:left">直接指定元素值</td>
<td style="text-align:left">x = torch.tensor([5.5, 0.02])</td>
</tr>
<tr>
<td style="text-align:left">维度变换</td>
<td style="text-align:left">x = y.view(-1,10)</td>
</tr>
<tr>
<td style="text-align:left">去掉个数为1的维度</td>
<td style="text-align:left">x = y.squeeze()</td>
</tr>
<tr>
<td style="text-align:left">one-hot encoding/decoding</td>
<td style="text-align:left">pytorch.scatter_(), pytorch.gather()</td>
</tr>
</tbody>
</table>
</li>
</ul>
<h3 id="223-tensor-和-numpy-转换">2.2.3 Tensor 和 Numpy 转换</h3>
<p>Tensor与numpy对象共享内存，但numpy只支持CPU，所以他们在CPU之间切换很快。但也意味着其中一个变化了，另外一个也会变。</p>
<ul>
<li>
<p>tensor 和 python 对象转换：<br>
<code>tensor.tolist()</code>：多个元素的tensor<br>
<code>tensor.item()</code>：只能用于含一个元素的tensor（标量）</p>
</li>
<li>
<p>Torch -&gt; NumPy:</p>
<pre><code class="language-python">a = torch.ones(5) # Torch Tensor
b = a.numpy() # NumPy Array
</code></pre>
</li>
<li>
<p>Numpy -&gt; Torch:</p>
<pre><code class="language-python">import numpy as np
a = np.ones(5) # NumPy Array
b = torch.from_numpy(a) # Torch Tensor
</code></pre>
</li>
<li>
<p>图像从<code>cv2.imread()</code>的<code>numpy</code>转换为可输入网络的<code>tensor</code>：<br>
cv2 numpy默认格式：<code>H*W*C, BGR</code>; torch tensor默认格式：<code>C*H*W, RGB</code></p>
<pre><code class="language-python">import cv2
import torchvision.transforms
image_np = cv2.imread(&quot;1.jpg&quot;)

# Method 1
image_tensor = transforms.Totensor()(image_np)

# Method 2
image_np = image_np[:, :, ::-1]
image_tensor = image_np.transpose((1,2,0))
</code></pre>
</li>
</ul>
<h3 id="224-在-cpu-和-gpu-之间移动数据">2.2.4 在 CPU 和 GPU 之间移动数据</h3>
<pre><code class="language-python"># move the tensor to GPU
x = x.to(&quot;cuda&quot;)  # or x = x.cuda()

# directly create a tensor on GPU
device = 'cuda' if torch.cuda.is_available() else 'cpu'
torch.manual_seed(42)
a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)
b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)

# move the tensor to CPU
x = x.to(&quot;cpu&quot;) # or x = x.cpu()
</code></pre>
<h3 id="225-detach-detach_-和-data-区别">2.2.5 <code>.detach()</code>, <code>.detach_()</code> 和 <code>.data</code> 区别</h3>
<blockquote>
<p>https://www.cnblogs.com/wanghui-garcia/p/10677071.html<br>
https://zhuanlan.zhihu.com/p/83329768</p>
</blockquote>
<ul>
<li>
<p>.detach() 和 .detach_()<br>
detach_() 是对 Variable 本身的更改，detach() 则是生成了一个新的 Variable</p>
</li>
<li>
<p>.detach() 和 .data 相同点：</p>
<ul>
<li>requires_grad 都为 false，即使之后重新将它的requires_grad置为true，它也不会具有梯度grad</li>
<li>都返回一个从当前计算图中分离下来的，新的Variable。但是仍指向原变量的存放位置，也即和原变量共享一块内存</li>
</ul>
</li>
<li>
<p>.detach() 和 .data 不同点：</p>
<ul>
<li>
<p><code>.detach()</code>之后修改会被autograd追踪，保证了只要在backward过程中没有报错，那么梯度的计算就是正确的</p>
</li>
<li>
<p><code>.data</code>之后的修改不会被autograd追踪，可能会产生错误的梯度，<strong>所以 .data 不够安全</strong>，用 <code>x.detach()</code> 更好</p>
<p>.detach() 之后不进行修改：</p>
<pre><code class="language-python">import torch

a = torch.tensor([1, 2, 3.], requires_grad=True)
out = a.sigmoid()
c = out.detach()

# 这时候没有对c进行更改，所以并不会影响backward()
out.sum().backward()
</code></pre>
<p>.detach() 之后进行in-place修改：</p>
<pre><code class="language-python">import torch

a = torch.tensor([1, 2, 3.], requires_grad=True)
out = a.sigmoid()
c = out.detach()

c.zero_()
print(c)  # tensor([0., 0., 0.])
print(out)  # tensor([0., 0., 0.], grad_fn=&lt;SigmoidBackward&gt;)
out.sum().backward()  # 报错
</code></pre>
<p>.data 之后进行修改</p>
<pre><code class="language-python">import torch

a = torch.tensor([1, 2, 3.], requires_grad=True)
out = a.sigmoid()
c = out.data

# 会发现c的修改同时也会影响out的值
c.zero_()
print(c, out)

# 不同之处在于.data的修改不会被autograd追踪，这样当进行backward()时它不会报错，会得到一个错误的backward值
out.sum().backward()
print(a.grad) # tensor([0., 0., 0.])
</code></pre>
</li>
</ul>
</li>
</ul>
<hr>
<p><br><br></p>
<h1 id="3-代码分析">3. 代码分析</h1>
<blockquote>
<p>参考:<br>
https://github.com/pytorch/tutorials<br>
https://github.com/pytorch/examples<br>
http://pytorch.org/docs/<br>
https://discuss.pytorch.org/</p>
</blockquote>
<h2 id="31-a-toy-example-of-back-propagation">3.1 A Toy Example of Back Propagation</h2>
<p>Only need to define the forward function, and the backward function is automatically defined.</p>
<ul>
<li>
<p>Define the network (step 1)</p>
<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
  def __init__(self):
    super(Net, self).__init__()
    self.conv1 = nn.Conv2d(1, 6, 5)
    self.conv2 = nn.Conv2d(6, 16, 5)
    self.fc1 = nn.Linear(16 * 5 * 5, 120)
    self.fc2 = nn.Linear(120, 84)
    self.fc3 = nn.Linear(84, 10)
    
  def forward(self, x):
    x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
    x = F.max_pool2d(F.relu(self.conv2(x)), 2)
    x = x.view(-1, 16 * 5 * 5)
    x = F.relu(self.fc1(x))
    x = F.relu(self.fc2(x))
    x = self.fc3(x)
return x

net = Net()
</code></pre>
</li>
<li>
<p>Process inputs (step 2)</p>
<pre><code class="language-python">input = torch.randn(1, 1, 32, 32)
out = net(input)
</code></pre>
</li>
<li>
<p>Compute the loss (step 3)</p>
<pre><code class="language-python">output = net(input)
target = torch.randn(10) # a dummy target, for example
target = target.view(1, -1) # reshape the dimension with -1 inferred from other dimensions (10/1=10, the same shape with input)
criterion = nn.MSELoss()
loss = criterion(output, target)
</code></pre>
</li>
<li>
<p>Backprop and update the weights (step 4)</p>
<pre><code class="language-python">import torch.optim as optim
optimizer = optim.SGD(net.parameters(), lr=0.01)    # optimizer obtains the references of parameters

optimizer.zero_grad()     # zero the gradient buffers
loss.backward()       # calculate the gradients of parameters
optimizer.step()    # Does the update
</code></pre>
</li>
</ul>
<br>
<h2 id="32-steps-to-train-a-classifier">3.2 Steps to Train a Classifier</h2>
<ul>
<li>
<p>Load the dataset (step 1)</p>
<pre><code class="language-python">import torch
import torchvision
import torchvision.transforms as transforms

transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))] )

trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)
</code></pre>
</li>
<li>
<p>Define the network. Same as before. (step 2)</p>
<pre><code class="language-python">import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
  def __init__(self):
    super(Net, self).__init__()
    self.conv1 = nn.Conv2d(1, 6, 5)
    self.conv2 = nn.Conv2d(6, 16, 5)
    self.fc1 = nn.Linear(16 * 5 * 5, 120)
    self.fc2 = nn.Linear(120, 84)
    self.fc3 = nn.Linear(84, 10)
    
  def forward(self, x):
    x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
    x = F.max_pool2d(F.relu(self.conv2(x)), 2)
    x = x.view(-1, 16 * 5 * 5)
    x = F.relu(self.fc1(x))
    x = F.relu(self.fc2(x))
    x = self.fc3(x)
    return x
    
net = Net()
</code></pre>
</li>
<li>
<p>Define the loss function and optimizer. Same as before. (step 3)</p>
<pre><code class="language-python">import torch.optim as optim

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
</code></pre>
</li>
<li>
<p>Train the network (step 4)</p>
<pre><code class="language-python">for epoch in range(2): # loop over the dataset multiple times

  running_loss = 0.0
  for i, data in enumerate(trainloader, 0):
  
  # get the inputs
  inputs, labels = data
  
  # zero the parameter gradients
  optimizer.zero_grad()
  
  # forward + backward + optimize
  outputs = net(inputs)
  loss = criterion(outputs, labels)
  loss.backward()
  optimizer.step()
  
  # print statistics
  running_loss += loss.item()
  if i % 2000 == 1999: # print every 2000 mini-batches
    print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 2000))
    running_loss = 0.0
</code></pre>
</li>
<li>
<p>Test the network (step 5)</p>
<pre><code class="language-python">correct = 0
total = 0
with torch.no_grad():          # 因为Pytorch会自动计算梯度，但这里明确告诉它不用计算梯度了
  for data in testloader:
    images, labels = data
    outputs = net(images)      # 这里output是torch.autograd.Variable的类型
    _, predicted = torch.max(outputs.data, 1)     # output.data才是tensor格式，1代表在哪个维度求最大值
    total += labels.size(0)    # labels.size()=1
    correct += (predicted == labels).sum().item()    
  
print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))
</code></pre>
</li>
<li>
<p>Options: Train on GPU/GPUs</p>
<blockquote>
<p>用多个GPU：<code>net = nn.DataParallel(net)</code></p>
</blockquote>
<pre><code class="language-python"># training on the first cuda device
device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
net.to(device)
inputs, labels = inputs.to(device), labels.to(device)
</code></pre>
</li>
</ul>
<br>
<h2 id="33-yolov3的实现摘要">3.3 Yolov3的实现摘要</h2>
<pre><code class="language-python">import torch.nn as nn
import torch.nn.functional as F 

def get_test_input():
    # 读取图片，得到torch变量(略)#
    return img_

def parse_cfg(cfgfile):
    #block格式是字典，代表网络中的一个Module (一个Module可能有多层)。blocks是block组成的列表，代表整个cfg文件。
    block = {}
    blocks = []
    # 此处要进行一些cfg文件读取操作(略)#
    return blocks

def create_modules(blocks):
    net_info = blocks[0]	#读取cfg文件中的[net]信息
    module_list = nn.ModuleList()	#module_list存储了用blocks构建的整个网络，module_list对应于blocks[1:]
    output_filters = []		#记录之前每个层的卷积核数量
    prev_filters = 3		#初始输入数据3通道。每次卷积都将prev_filters个通道变为filters个通道
    for index, x in enumerate(blocks[1:]): 	#x是一个字典，与block类似
        module = nn.Sequential()		#A module could have many layers (Conv, BN, ReLU...)
        if (x[&quot;type&quot;] == &quot;convolutional&quot;):	#卷积层
            # 添加卷基层
            conv = nn.Conv2d(prev_filters, filters, kernel_size, stride, pad, bias)
            module.add_module(&quot;conv_{0}&quot;.format(index), conv)		#Adds a child module to the current module
            #添加BN层
            bn = nn.BatchNorm2d(filters)
            module.add_module(&quot;batch_norm_{0}&quot;.format(index), bn)
            #其他层以此类推(略)#   
        elif (x[&quot;type&quot;] == &quot;upsample&quot;):	#上采样层
            #写法同卷基层(略)#     
        module_list.append(module)
        prev_filters = filters
        output_filters.append(filters)
    return (net_info, module_list)
</code></pre>

                </div>
            </article>
        </div>

        
            <div class="next-post">
                <div class="next gt-c-content-color-first">下一篇</div>
                <a href="https://kimokcheon.github.io/post/c-opencv/" class="post-title gt-a-link">
                    C++ OpenCV
                </a>
            </div>
        

        

        

        
            <script src='https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js'></script>

<style>
	div#vcomments{
		width:100%;
		max-width: 1000px;
		padding: 2.5%
	}
</style>


	<div id="vcomments"></div>

<script>
	new Valine({
		el: '#vcomments',
		appId: '',
		appKey: '',
		avatar: '',
		pageSize: 5,
		recordIp: false,
		placeholder: 'Just Go Go',
		visitor: false,
	});
</script>

        

        <div class="site-footer gt-c-content-color-first">
    <div class="slogan gt-c-content-color-first">他们都是萤火，聚在一起就成了太阳</div>
    <div class="social-container">
        
            
                <a href="https://github.com/Kimokcheon" target="_blank">
                    <i class="fab fa-github gt-c-content-color-first"></i>
                </a>
            
        
            
        
            
        
            
                <a href="https://www.zhihu.com/people/deng-yu-chuan-4" target="_blank">
                    <i class="fab fa-zhihu gt-c-content-color-first"></i>
                </a>
            
        
            
        
            
        
    </div>
    <div class="footer-info">
        Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a>
    </div>
    <div>
        Theme by <a href="https://imhanjie.com/" target="_blank">imhanjie</a>, Powered by <a
                href="https://github.com/getgridea/gridea" target="_blank">Gridea | <a href="https://kimokcheon.github.io//atom.xml" target="_blank">RSS</a></a>
    </div>
</div>

<script>
  hljs.initHighlightingOnLoad()
</script>

    </div>
</div>
</body>
</html>
